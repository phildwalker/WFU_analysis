---
title: 'Analysis: Understanding Student Major Selection (STEM Focus)'
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: true
    toc_float: true
    theme: spacelab
    includes:
      after_body: footer.html
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

<!-- cerulean, journal, flatly, darkly, readable, spacelab, united, cosmo, lumen, paper, sandstone, simplex, and yeti -->

```{css, echo=FALSE}
body .main-container {
  max-width: 100% !important;
  width: 100% !important;
    }
body {
  max-width: 100% !important;
    }
```


```{r read in data, results='asis', echo=FALSE, include=FALSE}
library(tidyverse)
library(ggthemes)
library(knitr)
library(viridis)
library(kableExtra)
library(modelr)
library(broom)
library(tidymodels)

# initialize function

round_df <- function(df, digits) {
  nums <- vapply(df, is.numeric, FUN.VALUE = logical(1))

  df[,nums] <- round(df[,nums], digits = digits)

  (df)
}


recipe_simple <- function(dataset) {
  recipe(STEM ~ ., data = dataset) %>%
    step_string2factor(all_nominal(), -all_outcomes()) %>%
    prep(data = dataset)
}

```

# Introduction {.tabset .tabset-pills}

### Goal 
We are looking to understand the relationship between what major a students chooses and that student's self assessment of the activities they engagement in, their values, and their perceived competencies in additional to their demographic characteristics (which could potentially serve an latent variables for cultural norms around STEM desirability).

***

<u>Per the prompt:</u>

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 10px;}
</style>
<div class = "blue">

##### _Knowing more about the characteristics of students who choose STEM majors can help us predict how many students will choose those majors and support the students in those majors._

</div>

***


### Assumptions Made

*  That the surveys are based on ‘probability sample’. (ie surveys where all studnets in our population have a chance of being selected and the probability of selection is known).
    - Believe this to a safe assumption because this survey went out to all students.
*  That the survey questions had already been validated to be used for this population. (ie the scales have been validated)

***

### Design Decisions  

At the end of this analysis, I wanted to produce a model that is <u>reproducible</u> and _extendable_ to other datases.    

*  With reproducibility in mind, I chose to perform the analysis within R and to document the process as well as the findings within [RMarkdown](https://rmarkdown.rstudio.com/)  
*  Where possible, I have sought to make this analysis portable and extendable through the folder project structure and code structure

***

### Investigation Reason  
If we can predict the major of the students and understand the driving factors there are a few actions that can be taken from the model:

1.  Looking for what characteristics are _"valued" within the model_:
    - "Optimize Enrollment": Ie accept more/less students who meet the model characteristics 
    - Adjust resources based upon currently available data (ie might see that there is a bubble of STEM students coming which might require more lab space)    
2.  Looking for what characteristics that are _not "valued" within the model_:  
    - Look to interview students who would have likely been within STEM, but due to a reason aren't within STEM

***

# EDA {.tabset .tabset-pills}

## Data Overview  
### Data Source  

Our data for this analysis came from two differnt sources:  

1.  The surevy results provided by the IR team.  
    - Students of the 2012, 2013, 2014, and 2015 freshman cohorts were surveyed during their second semester of their first year.  
2.  The factbooks for the corresponding years 

_Note: the factbooks were compiled from the Wake Forest Office of Institutional Research site [Link](https://ir.wfu.edu/)_

<center>
![FigName](images/WFU_OIR.PNG)
</center>

***
<u>Sample of the data that was extracted from the factbooks:</u>

<center>
![FigName](images/WFU_exFactBook.PNG)
</center>


### Data imported and additional factors created  

*  For this dataset I decided to transform the _scales of the survey questionaire_ to categorical data because I did not want to assume later on a consistent and proportional increase in value across the scale.  
*  In the review of the _Race/Ethnicity_ distribution both witin the fact book as well as the survey response data, I found that ~90% of all students reported their ethnicty to be either White, Asian, or Hispanic/LatinX. Therefore I collapsed the remaining ethnicities within an "Other" group.
*  I also decided to create a new variable called _STEM_, which output a YES/NO if the major for the student fell within the STEM category. This is benefitial when developing the initial model which seeks to provide a prediction of the student's major being either STEM or non-STEM.

```{r load the data}
# Load data
suppressWarnings(suppressMessages(
WFU_stu <- readr::read_csv("./Data/data_for_interviewees.csv") %>% 
  mutate_if(is.numeric,as.factor) %>%
  mutate(GENDER = trimws(toupper(GENDER)),
         STEM = case_when(MAJOR == 6 ~"YES-STEM", TRUE ~ "NO-STEM"),
         STEM = as.factor(STEM),
         FIRST_GEN = fct_collapse(FIRST_GEN, "YES" = c("2"), "NO" = c("1"))) %>%
  mutate(RACEgroup = fct_collapse(RACETHN, "Other" = c("1", "3", "5", "7"),
                                              "Asian" = c("2"),
                                              "LatinX" = c("4"),
                                              "White" = c("6"))) %>% 
  mutate(COLLEGE_GPA = fct_collapse(COLLEGE_GPA, "D or lower" = c("1"),
                                    "C" = c("2"),
                                    "C+" = c("3"),
                                    "B-" = c("4"),
                                    "B" = c("5"),
                                    "B+" = c("6"),
                                    "A-" = c("7"),
                                    "A or A+" = c("8"))) %>%  
  mutate(HS_GPA = fct_collapse(HS_GPA, "D or lower" = c("1"),
                                    "C" = c("2"),
                                    "C+" = c("3"),
                                    "B-" = c("4"),
                                    "B" = c("5"),
                                    "B+" = c("6"),
                                    "A-" = c("7"),
                                    "A or A+" = c("8"))) %>% 
  mutate(DISTANCE = fct_collapse(DISTANCE, "5 or less" = c("1"),
                                    "6 - 10" = c("2"),
                                    "11 - 50" = c("3"),
                                    "51 - 100" = c("4"),
                                    "101 - 500" = c("5"),
                                    "Over 500" = c("6"))) %>%
  ungroup()
))


EthLook <- readxl::read_excel("./Data/DataFromFactBooks.xlsx", sheet = "LookUp")

suppressWarnings(suppressMessages(
FB <- readxl::read_excel("./Data/DataFromFactBooks.xlsx", sheet = "FC_Data") %>% 
  gather(3:9, key= "year",value="StuCount") %>% 
  mutate(year = as.numeric(year),
         Gender= trimws(toupper(Gender))) %>% 
  mutate_if(is.numeric, replace_na, 0) %>% 
  left_join(., EthLook) %>% 
  mutate(ID = as_factor(ID),
    RACEgroup = fct_collapse(ID, "Other" = c("-1","1", "3", "5", "7"),
                                              "Asian" = c("2"),
                                              "LatinX" = c("4"),
                                              "White" = c("6")))
))


suppressWarnings(suppressMessages(
WFU_stu %>% group_by(YEAR) %>% summarise(CountStu_response = n()) %>% 
  left_join(., 
            (FB %>% mutate(year = as_factor(year)) %>% group_by(year) %>% summarise(TotalStu = sum(StuCount, na.rm=TRUE))),
            by= c("YEAR" = "year")) %>% 
  mutate(PercentResponse = CountStu_response/TotalStu) %>% 
  mutate(PercentResponse = scales::percent(PercentResponse)) %>% 
  kable(align='c') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", full_width = F))
))


```

_Note for future study: What happened within 2014 where the respose rate was substantially lower than the other years?_

### Overview of the provided dataset

```{r provide an overview of the data, out.width=c('50%', '50%'), fig.show='hold'}
visdat::vis_dat(WFU_stu, sort_type = FALSE, palette = "cb_safe") + 
  coord_flip() + 
  theme_bw()

visdat::vis_miss(WFU_stu) + coord_flip() + theme_bw()

```

## Demographic Comparison
__Compare survey response to student population using WFU fact book__  

The first we want look at is whether or not the survey respondents are in line with the student population during the same period of time.

Decided to use a Chi-square test of independence, which is commonly used for comparing categorical data. In our case we are testing if the proportion matches (which we would call our null hypothesis). If the p-value of the chi-square test is very small than we would reject the null hypothesis and say that our two populations are different.


### Chi-Square: Gender + Race/Ethnicity

__Looking into the comparison of gender and ethnicity__
```{r testing response demographics}
WFU_Resp <- WFU_stu %>% 
  group_by(GENDER, RACEgroup) %>% summarise(CountStu_Samp = n()) %>% 
  ungroup() %>% 
  mutate(Samp_Perc = CountStu_Samp/sum(CountStu_Samp))


TotalPop <- FB %>% filter(year <= 2015) %>% group_by(Gender, RACEgroup) %>% 
  summarise(CountStu_Pop = sum(StuCount, na.rm = TRUE)) %>% 
  ungroup() %>% 
  mutate(Pop_Perc = CountStu_Pop/sum(CountStu_Pop))

ComparSamp <- left_join(TotalPop, WFU_Resp,
                        by = c("Gender" = "GENDER", "RACEgroup" = "RACEgroup")) %>% 
  mutate_if(is.numeric, replace_na, 0) %>%
  mutate(NonRep_wt = Pop_Perc/Samp_Perc)

ComparSamp %>%
  round_df(2) %>% 
  kable(align='c') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", full_width = F))


chisq.test(ComparSamp$CountStu_Samp, p = ComparSamp$Pop_Perc)

## with a p-value ~0, means that the sample distribution doesn't match the population    



GEN_ETH_wt <- ComparSamp %>% select(Gender, RACEgroup, NonRep_wt)

```
Here, as with the two tests below, we find that the sample response rate does not represent the student population.

### Chi-Square: Gender

__Performing a chi-square test on the response counts of different genders__

```{r}
#chi square test for gender...
WFU_Resp <- WFU_stu %>% 
  group_by(GENDER) %>% summarise(CountStu_Samp = n()) %>% 
  ungroup() %>% 
  mutate(Samp_Perc = CountStu_Samp/sum(CountStu_Samp))

TotalPop <- FB %>% filter(year <= 2015) %>% group_by(Gender) %>% 
  summarise(CountStu_Pop = sum(StuCount, na.rm = TRUE)) %>% 
  ungroup() %>% 
  mutate(Pop_Perc = CountStu_Pop/sum(CountStu_Pop))

ComparSamp <- left_join(TotalPop, WFU_Resp,
                        by = c("Gender" = "GENDER")) %>% 
  mutate_if(is.numeric, replace_na, 0) %>% 
  mutate(NonRep_wt = Pop_Perc/Samp_Perc)

ComparSamp %>%
  round_df(2) %>% 
  kable(align='c') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", full_width = F))

chisq.test(ComparSamp$CountStu_Samp, p= ComparSamp$Pop_Perc)
## with a p-value ~0, means that the sample distribution doesn't match the population    

```
With a low p-value the Chi-square test, we reject the null hypothesis and determine that the survey response does not match the proportions of the student population for gender. 


### Chi-Square: Race/Ethnicity 

__Looking at the ethnicity comparison__

```{r}
WFU_Resp <- WFU_stu %>% 
  group_by(RACEgroup) %>% summarise(CountStu_Samp = n()) %>% 
  ungroup() %>% 
  mutate(Samp_Perc = CountStu_Samp/sum(CountStu_Samp),
         RACEgroup = RACEgroup)


TotalPop <- FB %>% filter(year <= 2015) %>% group_by(RACEgroup) %>% 
  summarise(CountStu_Pop = sum(StuCount, na.rm = TRUE)) %>% 
  ungroup() %>% 
  mutate(Pop_Perc = CountStu_Pop/sum(CountStu_Pop))

ComparSamp <- left_join(TotalPop, WFU_Resp,
                        by = c("RACEgroup" = "RACEgroup")) %>% 
  mutate_if(is.numeric, replace_na, 0) %>%
  mutate(NonRep_wt = Pop_Perc/Samp_Perc)

ComparSamp %>%
  round_df(2) %>%
  kable(align='c') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", full_width = F))

chisq.test(ComparSamp$CountStu_Samp, p = ComparSamp$Pop_Perc)


```
Chi-square would indicate that the survey response does not match the proportions of the student population for Ethnicity




## Non-response weights
By comparing the sample proportions to the population within the factbooks via a Chi-Square test we find that the __sample does not align with the population.__ 

This misalignment is likely due to _non-response_ and from calculating the non-response weight (NonRep_wt) we see that:  

-  Males for every ethnicity except for Asians responsed at a lower rate than the population (weight is >1)  
-  For all females that their response rate was higher than that of the population (weight is <1)

We will use these weights when reviewing our descriptive stats.

```{r weighting non-response}

GEN_ETH_wt %>% 
  arrange(desc(NonRep_wt)) %>% 
  round_df(., 3) %>% 
  kable(align='c') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", full_width = F))


```


## STEM vs Rest Respondents

Below is an comparison of the count of students who fall within each category.

### Compare response rates

__Demographic overview__


```{r comparing STEM vs Rest, fig.height = 4, fig.width = 10, fig.align = "center"}

plotDEMO <- function(col_name, TitleNM){
  col_name <- enquo(col_name)

  WFU_stu %>% 
    # select(2:9, STEM, RACEgroup) %>% 
  left_join(., GEN_ETH_wt, by = c("GENDER" = "Gender", "RACEgroup" = "RACEgroup")) %>% 
  group_by(STEM, !!col_name) %>% 
  summarise(WeightedResp = sum(NonRep_wt, na.rm = TRUE)) %>% 
  ungroup() %>% 
  group_by(STEM) %>% 
  mutate(TotalRes = sum(WeightedResp),
         Percent = WeightedResp/TotalRes) %>% 
  ggplot(aes(x=!!col_name, y=Percent))+
    geom_bar(stat = "identity", aes(fill=!!col_name))+
    geom_text(aes(label = scales::percent(Percent),
                   y= Percent ), stat= "identity", vjust = -.5) + #, size=3
    theme_bw()+
    scale_fill_viridis(discrete=TRUE) +
    facet_grid( ~ STEM) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1), breaks=seq(0, 1, by=0.2)) + 
    expand_limits(y=c(0,1)) + 
    ggtitle(paste0(TitleNM, " percentage by STEM group [Adjusted for non-response]"))
  }

plotDEMO(GENDER, "Gender")    
plotDEMO(RACEgroup, "Aggregated Ethnicity")    
plotDEMO(YEAR, "Student Cohort")    
plotDEMO(FIRST_GEN, "First Generation") 
suppressWarnings(suppressMessages(plotDEMO(DISTANCE, "Distance from Home")))
suppressWarnings(suppressMessages(plotDEMO(HS_GPA, "HS GPA"))) 
plotDEMO(COLLEGE_GPA, "College GPA")



```


**Responses to ACT Questions**
These questions asked students about the frequency at which they performed certain activities in the past year. The questions spanned questions about their activity outside of the classroom environment to inside the classroom. 

<u>Scale</u>  
1: Not at all  
2: Occasionally  
3: Frequently  

Based on my personal bias (as a former STEM undergrad) I would expect the questions that would be most likley to be positively associated with STEM selection are:  

ACT09: _Look up scientific research articles and resources_   
ACT13: _Write computer code_



```{r comparing STEM vs Rest ACT, fig.height = 2, fig.width = 12, fig.align = "center"}

plotDEMO(ACT01, "ACT01") + xlab("Score") + theme(legend.position = "none")
plotDEMO(ACT02, "ACT02") + xlab("Score") + theme(legend.position = "none")
plotDEMO(ACT03, "ACT03") + xlab("Score") + theme(legend.position = "none")
plotDEMO(ACT04, "ACT04") + xlab("Score") + theme(legend.position = "none")
plotDEMO(ACT05, "ACT05") + xlab("Score") + theme(legend.position = "none")
plotDEMO(ACT06, "ACT06") + xlab("Score") + theme(legend.position = "none")
plotDEMO(ACT07, "ACT07") + xlab("Score") + theme(legend.position = "none")
plotDEMO(ACT08, "ACT08") + xlab("Score") + theme(legend.position = "none")
plotDEMO(ACT09, "ACT09") + xlab("Score") + theme(legend.position = "none")
plotDEMO(ACT10, "ACT10") + xlab("Score") + theme(legend.position = "none")
plotDEMO(ACT11, "ACT11") + xlab("Score") + theme(legend.position = "none")
plotDEMO(ACT12, "ACT12") + xlab("Score") + theme(legend.position = "none")
plotDEMO(ACT13, "ACT13") + xlab("Score") + theme(legend.position = "none")


```

__Responses to RATE Questions__

These questions asked students about their perception of competence in a few areas. This group of questions fell into 2 general categories. 1) Student perception of interpersonal interactions and 2) student perception of their own academic related compentencies (like mathematical or writing abilities)

<u>Scale</u>  
1: A major weakness  
2: somewhat weak  
3: Average  
4: Somewhat strong  
5: A major strength  

Based on my personal bias, I would expect the questions that would be most likley to be positively associated with STEM selection are:  

RATE6: _Mathematical ability_  

I would also expect the sterotype of poor writing ability (RATE7) to potentially result in a negative association to STEM.

```{r comparing STEM vs Rest RATE, fig.height = 2, fig.width = 12, fig.align = "center"}

plotDEMO(RATE1, "RATE1") + xlab("Score") + theme(legend.position = "none")
plotDEMO(RATE2, "RATE2") + xlab("Score") + theme(legend.position = "none")
plotDEMO(RATE3, "RATE3") + xlab("Score") + theme(legend.position = "none")
plotDEMO(RATE4, "RATE4") + xlab("Score") + theme(legend.position = "none")
plotDEMO(RATE5, "RATE5") + xlab("Score") + theme(legend.position = "none")
plotDEMO(RATE6, "RATE6") + xlab("Score") + theme(legend.position = "none")
plotDEMO(RATE7, "RATE7") + xlab("Score") + theme(legend.position = "none")



```

__Responses to AGREE Questions__

These questions asked students about how they would assess the value of certain future goals or their perception of what community they belong to.   

<u>Scale</u>  
1: Strongly disagree  
2: Disagree  
3: Slightly disagree  
4: Slightly agree  
5: Agree  
6: Strongly agree

Based on my personal bias, I would expect the questions that would be most likley to be positively associated with STEM selection are:  

AGREE_01: _I have a strong sense of belonging to a community of scientists_  
AGREE_02: _I think of myself as a scientist_  
AGREE_03: _I feel like I belong in the field of science_   
AGREE_08: _Making a theoretical contribution to science is important to me_   



```{r comparing STEM vs Rest AGREE, fig.height = 2, fig.width = 12, fig.align = "center"}
plotDEMO(AGREE_01, "AGREE_01") + xlab("Score") + theme(legend.position = "none")
plotDEMO(AGREE_02, "AGREE_02") + xlab("Score") + theme(legend.position = "none")
plotDEMO(AGREE_03, "AGREE_03") + xlab("Score") + theme(legend.position = "none")
plotDEMO(AGREE_04, "AGREE_04") + xlab("Score") + theme(legend.position = "none")
plotDEMO(AGREE_05, "AGREE_05") + xlab("Score") + theme(legend.position = "none")
plotDEMO(AGREE_06, "AGREE_06") + xlab("Score") + theme(legend.position = "none")
plotDEMO(AGREE_07, "AGREE_07") + xlab("Score") + theme(legend.position = "none")
plotDEMO(AGREE_08, "AGREE_08") + xlab("Score") + theme(legend.position = "none")
plotDEMO(AGREE_09, "AGREE_09") + xlab("Score") + theme(legend.position = "none")


```

## Missingness Notes
Based on the descriptive statistics review of the variables, the missing data for "DISTANCE" and "HS_GPA"   





# Model {.tabset .tabset-pills}
## Develop Model for Major
The goal of the model is to be able to predict a binary outcome that is categorical (is the student likely to choose STEM or not as their major). These types of models are generally called "classification models" and two commonly used ones are Logistic regessions and random forests.

***

### Logisitic Regression
In this algorithm, the probabilities describing the possible outcomes of a single trial are modelled using a regession line which is then transformed via log. This algorithm is a part of the generalized linear model (glm) family, of which linear regession is a part.

Advantages: Logistic regression was designed for classification, and is very useful for understanding the influence of several independent variables on a single outcome variable.

Disadvantages: Works only when the predicted variable is binary, assumes all predictors are independent of each other, and assumes data is free of missing values.

***

### Random Forest
Random forest classifier fits a number of decision trees on various sub-samples of datasets and uses average to improve the predictive accuracy of the model and control over-fitting.

Advantages: Reduction in over-fitting and random forest classifier is more accurate than decision trees in most cases.

Disadvantages: Slow real time prediction, difficult to implement, and complex algorithm.

***

### Model Overview

<u>Training and Test data</u>

We developing models, often we will split the data into "training" and "test" datasets. This allows to have a dataset which was not used for the creation be used when we test our model accuracy.  
In the case of this model we have utilized a 75/25 (training/test) split.
	
<u>Model Performance</u>

*  Accuracy: (True Positive + True Negative) / Total Population  
*  Precision: "how sensitive models are to False Positives" (i.e. predicting a student will not choose STEM is leaving when he-she actually chooses STEM)  
*  Recall: "how sensitive models are to False Negatives" (i.e. predicting that a student will choose STEM he-she will not choose STEM).

All of these outputs can be calculated using a _confusion matrix_ which visually allows us to see how many students are correctly assigned (true positive and true negative) vs how many aren't when we use our model with the test dataset.

***

```{r model prep}

WFU_model <- WFU_stu %>% 
  # mutate(STEM = case_when(MAJOR == 6 ~"YES", TRUE ~ "NO")) %>% 
  # select(-MAJOR, -STUID, -YEAR, -DISTANCE, -HS_GPA) %>% 
  select(STEM, GENDER, RACEgroup, COLLEGE_GPA, FIRST_GEN, 
         ACT02, ACT08, ACT09, ACT12, ACT13, 
         RATE3, RATE5, RATE7, 
         AGREE_01, AGREE_02, AGREE_03, AGREE_08, AGREE_06, AGREE_07
         ) %>%
  mutate(GENDER = as_factor(GENDER),
         STEM = as_factor(STEM))

# Using tidymodels structure ---------

train_test_split <-
  rsample::initial_split(
    data = WFU_model,     
    prop = 0.75   
  ) 

# train_test_split

train_tbl <- train_test_split %>% training() 
test_tbl  <- train_test_split %>% testing()

recipe_prepped <- recipe_simple(dataset = train_tbl)

train_baked <- bake(recipe_prepped, new_data = train_tbl)
test_baked  <- bake(recipe_prepped, new_data = test_tbl)

```

## Logistic Regression  


```{r build Logistic}

# logGLM <- glm(STEM ~  ., data=train_baked, family="binomial")

logistic_glm <-
  logistic_reg(mode = "classification") %>%
  set_engine("glm") %>%
  fit(STEM ~ ., data = train_baked)

# logistic_glm

predictions_glm <- logistic_glm %>%
  predict(new_data = test_baked) %>%
  bind_cols(test_baked %>% select(STEM))




predictions_glm %>%
  metrics(STEM, .pred_class) %>%
  select(-.estimator) %>%
  filter(.metric == "accuracy") %>% 
    round_df(., 3)

# Precision shows how sensitive models are to False Positives (i.e. predicting a customer is leaving when he-she is actually staying) whereas 
# Recall looks at how sensitive models are to False Negatives (i.e. forecasting that a customer is staying whilst he-she is in fact leaving).

tibble(
  "precision" =
    precision(predictions_glm, STEM, .pred_class) %>%
    select(.estimate),
  "recall" = 
    recall(predictions_glm, STEM, .pred_class) %>%
    select(.estimate)
) %>%
  unnest() %>%
  round_df(., 3) %>% 
  kable(align='c') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", full_width = F))


predictions_glm %>% 
  mutate(Prediction = ifelse(.pred_class == "YES-STEM", 1, 0)) %>% 
  arrange(Prediction) %>%  
  mutate(ord = row_number()) %>% 
  ggplot(aes(x=ord, y=Prediction))+
    geom_point(aes(color=STEM), size =2, alpha = .5)+
    ylab("Predicted STEM")+
    xlab("")+
    scale_y_continuous(breaks=seq(0, 1, by=1)) + 
    expand_limits(y=c(0,1)) + 
    theme_bw()+
    viridis::scale_color_viridis(discrete=TRUE)+
    ggtitle("Logistic Regression Plot [Prediction vs Actual for Test data]")
  

```

### Odds Ratio 
utilizing odds ratio to interpret the elements of the logisitic model 

```{r log odds ratio, fig.height = 10, fig.width = 12, fig.align = "center"}


tidy(logistic_glm) %>% 
  mutate(pval = case_when(p.value < 0.05 ~ "<0.05", TRUE ~ ">0.05"),
         LogOdds = exp(estimate)) %>% 
  arrange(desc(abs(LogOdds))) %>% 
    round_df(3) %>% 
  ggplot(aes(x=fct_reorder(term, (estimate)), y=estimate))+
    geom_point(aes(color=pval))+
    geom_segment(aes(x = term, y = 0, xend = term, yend = estimate), color = "grey50") +
    geom_hline(yintercept = 0)+
    coord_flip()+
    theme_bw()+
    viridis::scale_color_viridis(discrete = TRUE)+
    xlab("")+
    ggtitle("Coefficient Plot: Logistic Regression")
    



tidy(logistic_glm) %>% 
  mutate(pval = case_when(p.value < 0.05 ~ "<0.05", TRUE ~ ">0.05"),
         Odds = exp(estimate)) %>% 
  filter(pval == "<0.05") %>%
  round_df(2) %>% 
  rename(LogOdds = estimate) %>%
  arrange(desc(abs(Odds))) %>% 
  kable(align='c') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", full_width = F))

# the odds of getting into an honors class for females (female = 1)over the odds of getting into an honors class for males (female = 0) is exp(.979948) = 2.66. 


```


## Random Forest
Utilizing a random forest model to classify if a student will be graduating within a STEM major or not.

```{r random forest load, results='asis', echo=FALSE, include=FALSE}
library(randomForest)
library(randomForestExplainer)
```

```{r tidy model rf}
# WFU_RF <- WFU_stu %>% 
#   # mutate(STEM = case_when(MAJOR == 6 ~"YES", TRUE ~ "NO")) %>% 
#   select(-MAJOR, -STUID, -YEAR, -DISTANCE, -HS_GPA, -RACETHN, -DISTANCEval) %>% 
#   # select(STEM, GENDER, RACETHN, COLLEGE_GPA, FIRST_GEN, ACT09, ACT12, ACT13, RATE5, RATE7, AGREE_01, AGREE_02, AGREE_03, AGREE_08) %>% 
#   mutate(GENDER = as_factor(GENDER),
#          STEM = as_factor(STEM))
# 
# rf_train_test_split <-
#   rsample::initial_split(
#     data = WFU_RF,     
#     prop = 0.80   
#   ) 
# 
# # train_test_split
# 
# rf_train_tbl <- rf_train_test_split %>% training() 
# rf_test_tbl  <- rf_train_test_split %>% testing()
# 
# rf_recipe_prepped <- recipe_simple(dataset = rf_train_tbl)
# 
# rf_train_baked <- bake(recipe_prepped, new_data = rf_train_tbl)
# rf_test_baked  <- bake(recipe_prepped, new_data = rf_test_tbl)

rf_build <- 
  rand_forest(trees = 1000, mtry = 6, mode = "classification") %>%
  set_engine("randomForest", importance=T, localImp = T) %>%
  fit(STEM ~ ., data = train_baked)


predictions_rf <- rf_build %>%
  predict(new_data = test_baked) %>%
  bind_cols(test_baked %>% select(STEM))


predictions_rf %>%
  metrics(STEM, .pred_class) %>%
  select(-.estimator) %>%
  filter(.metric == "accuracy") %>% 
    round_df(., 3) %>% 
  kable(align='c') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", full_width = F))


impt_frame <- measure_importance(rf_build$fit)

plot_multi_way_importance(impt_frame, no_of_labels = 10)
plot_multi_way_importance(impt_frame, x_measure = "accuracy_decrease", y_measure = "gini_decrease", size_measure = "p_value", no_of_labels = 10)


#variable depth
md_frame <- min_depth_distribution(rf_build$fit)

plot_min_depth_distribution(md_frame, mean_sample = "top_trees") # default mean_sample arg


```



## Confusion Matrices
The confusion matrix allows us to show (based on the test data) how often we are correctly predicting each category.  

When we compare the logistic regression vs the random forest we see that the __logistic regression__ model performs better than the random forest model. The logistic regession model also had the added benefit of being significantly easier to interpret than the random forest.

```{r confusion matrices, out.width=c('50%', '50%'), fig.show='hold'}
# Confusion matrix for Logistic
predictions_glm %>%
  conf_mat(STEM, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8)+
  theme_bw()+
  ggtitle("Confusion Plot: Logistic Model")





# Confusion matrix for RF
predictions_rf %>%
  conf_mat(STEM, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8)+
  theme_bw()+
  ggtitle("Confusion Plot: Random Forest Model")

```


# Takeaways/ Application {.tabset .tabset-pills}


## How to use the model

So that are the student characteristics that make them more likely to choose STEM as their major?

*  On a demographic standpoint they are more likely to be:  
    - male
    - white or asian
    - have a fairly high college gpa
*  On a student characteristic standpoint they are more likely to:  
    - believe themselves to be part of a scientific community (AGREE_01)
    - feel like they belong in the field of science (AGREE_03)
    - want to make theoretical contributions to scientific fields (AGREE_08)
    - think of themselves as a scientist (AGREE_02)


<u>If we want to optimize the current model</u>

1.  Would probably want to recruit from scientific communities.  
2.  With marketing, could highlight Wake's strong STEM majors in communities known to be more predominately white/asain


<u>If we want to correct for reasons why predictive elements</u>

1.  Start to interview female students or students from other ethnicities to explore more about why they didn't choose STEM.  
2.  Investigate why developing a life philosophy (AGREE_06) or being a community leader (AGREE_07) isn't important to students when choosing STEM. This could be more of how STEM is culturally understoon.


<u>Research Items</u>

1.  Seeing that "College GPA" has a fairly large impact in the model, a next step in this investigation would be to see if the types of classes that students took impacted their GPA.
2.  Leading vs lagging predictors: ie not sure if students chose STEM because they had a community or because they joined STEM they felt like they had a STEM community.

## Limitations
*  Did not review the students who changed their major later in their academic career, which could be used a test cases for how to transition the current population into STEM
*  Data doesn't show which students graduate within STEM. THis would help to understand if the support structure could be improved for those majors.
*  Assuming that the count of students from the fact book are representative of students from the survery (start of year vs 2nd semester)
*  Survey response distribution does not match with the student population 
*  Did not investigate the characterize the students who were admitted but did not enroll, which could be an opportunity to quickly increase the STEM enrollment

